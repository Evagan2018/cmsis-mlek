{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Machine Learning Evaluation Kit (MLEK) for CMSIS Workflows This documentation explains the usage of the CMSIS-MLEK Reference Application Package for developing Machine Learning (ML) and Edge AI applications on Cortex-M processors optional with Ethos-U NPUs. Target Audience This user's guide assumes basic knowledge about Cortex-M software development and CMSIS-Toolbox workflows. It is written for embedded software developers who work with C/C++ compiler toolchains and develop machine learning applications for microcontroller devices with Cortex-M processors and Ethos-U NPUs. Manual Chapters Overview explains the features and capabilities of the MLEK templates package. MLEK Audio Applications describes the available ML templates for audio use-cases. MLEK Video Applications describes the available ML templates for video and image use-cases. Targeting Hardware Boards gives step-by-step guidance for deploying the reference applications to a hardware platform in VS Code. Targeting Arm Virtual Hardware shows how to select a virtual target in Visual Studio code. Revision History Version Description 0.5.0 First published release.","title":"Home"},{"location":"index.html#machine-learning-evaluation-kit-mlek-for-cmsis-workflows","text":"This documentation explains the usage of the CMSIS-MLEK Reference Application Package for developing Machine Learning (ML) and Edge AI applications on Cortex-M processors optional with Ethos-U NPUs.","title":"Machine Learning Evaluation Kit (MLEK) for CMSIS Workflows"},{"location":"index.html#target-audience","text":"This user's guide assumes basic knowledge about Cortex-M software development and CMSIS-Toolbox workflows. It is written for embedded software developers who work with C/C++ compiler toolchains and develop machine learning applications for microcontroller devices with Cortex-M processors and Ethos-U NPUs.","title":"Target Audience"},{"location":"index.html#manual-chapters","text":"Overview explains the features and capabilities of the MLEK templates package. MLEK Audio Applications describes the available ML templates for audio use-cases. MLEK Video Applications describes the available ML templates for video and image use-cases. Targeting Hardware Boards gives step-by-step guidance for deploying the reference applications to a hardware platform in VS Code. Targeting Arm Virtual Hardware shows how to select a virtual target in Visual Studio code.","title":"Manual Chapters"},{"location":"index.html#revision-history","text":"Version Description 0.5.0 First published release.","title":"Revision History"},{"location":"overview.html","text":"Overview The Machine Learning Evaluation Kit (MLEK) pack contains CMSIS Reference Applications and templates for Edge AI development with embedded systems. These applications implement data preprocessing, memory management, and neural network inference pipelines that are optimized for Cortex-M and Ethos-U platforms. Key Features: Rapid Prototyping : Get working ML applications running quickly with minimal setup. Algorithm Development : Use example code as design patterns for custom ML algorithm implementation. Model Integration : Easily swap in custom TensorFlow Lite models with minimal code changes. Performance Validation : Test and optimize ML performance on target hardware or simulation. Hardware Evaluation : Compare performance across different Corstone platforms and configurations. The CMSIS-MLEK software pack is derived from the Arm\u00ae ML embedded evaluation kit and makes the examples easier to access. It also contains interfaces to physical hardware and simplifies porting to target hardware. It contains the following ML applications and uses currently Neural Network Models currently in TensorFlow Lite format. ML application Description Neural Network Model Keyword spotting (KWS) Recognize the presence of a key word in verbal speech MicroNet Object detection Detects and draws face bounding box in a given image Yolo Fastest Generic inference runner Code block allowing you to develop your own use case Your custom model Each ML reference application is a csolution project which supports deployment to physical hardware or Arm Virtual Hardware (AVH-FVP) for simulation. A board layer ( *.clayer.yml ) implements the drivers for the physical interfaces. The API interfaces required by the different applications is shown in the table below. Required API Interfaces Description Audio Processing CMSIS_VSTREAM_AUDIO_IN CMSIS-Driver vStream configured for Audio input. STDOUT Standard I/O for printf output. Video Processing CMSIS_VSTREAM_VIDEO_IN CMSIS-Driver vStream configured for Video input. CMSIS_VSTREAM_VIDEO_OUT CMSIS-Driver vStream configured for Video output. STDOUT Standard I/O for printf output. Generic Inference Runner STDOUT Standard I/O for printf output. Platform Support The templates support via target names multiple Arm Cortex-M IP Subsystems . These target names support execution on AVH FVP simulation models which is useful during software development or with Contiguous Integration (CI) testing using GitHub actions. Target Name IP Subsystem Description AVH-SSE-300 Corstone-300 Cortex-M55 optional with Ethos-U55 or Ethos-U65 AVH-SSE-310 Corstone-310 Cortex-M85 optional with Ethos-U55 AVH-SSE-315 Corstone-315 Cortex-M85 optional with Ethos-U65 AVH-SSE-320 Corstone-320 Cortex-M85 optional with Ethos-U85 Adding a postfix to the target name in the *.csolution.yml project file configures the neural network inference pipeline for Ethos-U. Without this prefix only the Cortex-M system is used as shown in the diagram below. Such a postfix can also be used for target names that deploy to physical hardware. Postfix Description none Cortex-M system only, no Ethos-U NPU -U55-128 Cortex-M system + Ethos-U55 NPU (128 MACs) -U55 Cortex-M system + Ethos-U55 NPU (256 MACs) -U65 Cortex-M system + Ethos-U65 NPU -U85 Cortex-M system + Ethos-U85 NPU","title":"Overview"},{"location":"overview.html#overview","text":"The Machine Learning Evaluation Kit (MLEK) pack contains CMSIS Reference Applications and templates for Edge AI development with embedded systems. These applications implement data preprocessing, memory management, and neural network inference pipelines that are optimized for Cortex-M and Ethos-U platforms. Key Features: Rapid Prototyping : Get working ML applications running quickly with minimal setup. Algorithm Development : Use example code as design patterns for custom ML algorithm implementation. Model Integration : Easily swap in custom TensorFlow Lite models with minimal code changes. Performance Validation : Test and optimize ML performance on target hardware or simulation. Hardware Evaluation : Compare performance across different Corstone platforms and configurations. The CMSIS-MLEK software pack is derived from the Arm\u00ae ML embedded evaluation kit and makes the examples easier to access. It also contains interfaces to physical hardware and simplifies porting to target hardware. It contains the following ML applications and uses currently Neural Network Models currently in TensorFlow Lite format. ML application Description Neural Network Model Keyword spotting (KWS) Recognize the presence of a key word in verbal speech MicroNet Object detection Detects and draws face bounding box in a given image Yolo Fastest Generic inference runner Code block allowing you to develop your own use case Your custom model Each ML reference application is a csolution project which supports deployment to physical hardware or Arm Virtual Hardware (AVH-FVP) for simulation. A board layer ( *.clayer.yml ) implements the drivers for the physical interfaces. The API interfaces required by the different applications is shown in the table below. Required API Interfaces Description Audio Processing CMSIS_VSTREAM_AUDIO_IN CMSIS-Driver vStream configured for Audio input. STDOUT Standard I/O for printf output. Video Processing CMSIS_VSTREAM_VIDEO_IN CMSIS-Driver vStream configured for Video input. CMSIS_VSTREAM_VIDEO_OUT CMSIS-Driver vStream configured for Video output. STDOUT Standard I/O for printf output. Generic Inference Runner STDOUT Standard I/O for printf output.","title":"Overview"},{"location":"overview.html#platform-support","text":"The templates support via target names multiple Arm Cortex-M IP Subsystems . These target names support execution on AVH FVP simulation models which is useful during software development or with Contiguous Integration (CI) testing using GitHub actions. Target Name IP Subsystem Description AVH-SSE-300 Corstone-300 Cortex-M55 optional with Ethos-U55 or Ethos-U65 AVH-SSE-310 Corstone-310 Cortex-M85 optional with Ethos-U55 AVH-SSE-315 Corstone-315 Cortex-M85 optional with Ethos-U65 AVH-SSE-320 Corstone-320 Cortex-M85 optional with Ethos-U85 Adding a postfix to the target name in the *.csolution.yml project file configures the neural network inference pipeline for Ethos-U. Without this prefix only the Cortex-M system is used as shown in the diagram below. Such a postfix can also be used for target names that deploy to physical hardware. Postfix Description none Cortex-M system only, no Ethos-U NPU -U55-128 Cortex-M system + Ethos-U55 NPU (128 MACs) -U55 Cortex-M system + Ethos-U55 NPU (256 MACs) -U65 Cortex-M system + Ethos-U65 NPU -U85 Cortex-M system + Ethos-U85 NPU","title":"Platform Support"},{"location":"target_configuration_avh.html","text":"Target Configuration Arm Virtual Hardware Supported Platforms The MLEK templates support multiple virtual Arm Cortex-M platforms with Ethos-U NPU acceleration: Platform Processor NPU Options Reference Application Support Corstone-300 Cortex-M55 Ethos-U55, Ethos-U65, No NPU All Corstone-310 Cortex-M85 Ethos-U55, Ethos-U65, No NPU All Corstone-315 Cortex-M85 Ethos-U65 All Corstone-320 Cortex-M85 Ethos-U85 All These targets are preconfigured in the solution and allow to run the applications without hardware. Using VS Code Open the Manage Solution dialog from the CMSIS Solution Extension window (see user guide for details). You can select an arbitrary FVP platform. Refer to the the above table, to find out which Cortex-M CPU core and which Ethos-U NPU will be simulated. Select the Build Types *-Data_Array to create an application image that includes test data for the model. A run configuration will automatically be created, so you can start the model and the application in the internal VSCode terminal, with the Run feature.","title":"Targeting Arm Virtual Hardware"},{"location":"target_configuration_avh.html#target-configuration-arm-virtual-hardware","text":"","title":"Target Configuration Arm Virtual Hardware"},{"location":"target_configuration_avh.html#supported-platforms","text":"The MLEK templates support multiple virtual Arm Cortex-M platforms with Ethos-U NPU acceleration: Platform Processor NPU Options Reference Application Support Corstone-300 Cortex-M55 Ethos-U55, Ethos-U65, No NPU All Corstone-310 Cortex-M85 Ethos-U55, Ethos-U65, No NPU All Corstone-315 Cortex-M85 Ethos-U65 All Corstone-320 Cortex-M85 Ethos-U85 All These targets are preconfigured in the solution and allow to run the applications without hardware.","title":"Supported Platforms"},{"location":"target_configuration_avh.html#using-vs-code","text":"Open the Manage Solution dialog from the CMSIS Solution Extension window (see user guide for details). You can select an arbitrary FVP platform. Refer to the the above table, to find out which Cortex-M CPU core and which Ethos-U NPU will be simulated. Select the Build Types *-Data_Array to create an application image that includes test data for the model. A run configuration will automatically be created, so you can start the model and the application in the internal VSCode terminal, with the Run feature.","title":"Using VS Code"},{"location":"target_configuration_refapp.html","text":"Target Configuration Reference Application Hardware Using VS Code This section explains how to use MLEK reference applications with the Arm CMSIS Solution extension for VS Code. Install Required Packs Install the CMSIS-MLEK pack and any required board support packs: cpackget add ARM::cmsis-mlek Create New Solution Open VS Code and use the Create a new solution dialog Select one of the MLEK reference applications: MLEK Keyword Spotting and Audio User Algorithm Template MLEK Object Detection and Video User Algorithm Template MLEK Generic Inference Runnner . (available in pack version 1.0+) Configure the target platform and toolchain: Build and Run Refer to the Arm CMSIS CSolution extentions documentation on how to use the CMSIS View. It offers convenient and pre-configured access to build and debug features. Enable Ethos-U NPU support for your target To enable NPU support for custom hardware: Open the *.csolution.yml file and look for the first target-type : target-types: - type: MyCustomBoard device: STM32U585AIIx # Your target MCU variables: ... To the \"type\" entry add a suffix that identifies NPU model and MAC configuration. e.g.: - type: MyCustomBoard-U55-128 # or - type: MyCustomBoard-U85-512 If you do not specify a MAC variant, 256 will be the default. This will configure the project to include the correct drivers and models for the NPU selected. Reference Applications Audio Template: Keyword Spotting The KWS template demonstrates real-time wake word detection: Key Features: Real-time audio preprocessing (MFCC feature extraction) Optimized neural network inference using CMSIS-NN Configurable wake word models Performance profiling and benchmarking Getting Started: Build and run the template on your target platform Speak the wake word (e.g. \"Yes\" or \"Up\") Observe detection results via UART output or LEDs Replace the model with your custom wake word model Customization Points: kws/src/kws_model.cpp : Replace with your TensorFlow Lite model kws/src/audio_preprocessing.cpp : Modify audio preprocessing pipeline kws/config/kws_config.h : Adjust detection thresholds and parameters Video Template: Object Detection The object detection template provides real-time computer vision: Key Features: Camera input processing and frame buffering Object detection using MobileNet-based models Bounding box visualization Multi-object detection and classification Getting Started: Build and run the template with camera input Point camera at objects for detection View detection results on display or via debug output Integrate your custom object detection model Customization Points: object-detection/src/detection_model.cpp : Replace with your model object-detection/src/image_preprocessing.cpp : Modify image preprocessing object-detection/config/detection_config.h : Adjust detection parameters Generic Template: Inference Runner (available in pack version 1.0+) The generic inference template provides maximum flexibility: Key Features: Framework for any TensorFlow Lite model Configurable input/output tensors Performance benchmarking utilities Extensible architecture for custom applications Getting Started: Replace the example model with your TensorFlow Lite model Configure input/output tensor specifications Implement custom preprocessing/postprocessing Build and test your custom ML application Customization Points: inference_runner/Model/ : Replace with your TensorFlow Lite model files inference_runner/src/inference_runner.cpp : Modify inference pipeline inference_runner/config/ : Adjust model and application configuration Performance Optimization All MLEK templates include built-in performance optimization features: CMSIS-NN Integration : Optimized neural network kernels for Cortex-M Ethos-U Acceleration : NPU acceleration for supported layers Memory Optimization : Efficient memory management and tensor allocation Profiling Tools : Built-in timing and resource usage measurements Next Steps After successfully running an MLEK template: Model Integration : Replace the example model with your trained TensorFlow Lite model Application Customization : Modify the application logic for your specific use case Performance Tuning : Optimize for your target constraints (memory, power, latency) Hardware Deployment : Test on your target hardware platform Production Deployment : Integrate into your final product design For detailed examples and additional resources, refer to the individual template README files and the MLEK documentation.","title":"Targeting Hardware Boards"},{"location":"target_configuration_refapp.html#target-configuration-reference-application-hardware","text":"","title":"Target Configuration Reference Application Hardware"},{"location":"target_configuration_refapp.html#using-vs-code","text":"This section explains how to use MLEK reference applications with the Arm CMSIS Solution extension for VS Code.","title":"Using VS Code"},{"location":"target_configuration_refapp.html#install-required-packs","text":"Install the CMSIS-MLEK pack and any required board support packs: cpackget add ARM::cmsis-mlek","title":"Install Required Packs"},{"location":"target_configuration_refapp.html#create-new-solution","text":"Open VS Code and use the Create a new solution dialog Select one of the MLEK reference applications: MLEK Keyword Spotting and Audio User Algorithm Template MLEK Object Detection and Video User Algorithm Template MLEK Generic Inference Runnner . (available in pack version 1.0+) Configure the target platform and toolchain:","title":"Create New Solution"},{"location":"target_configuration_refapp.html#build-and-run","text":"Refer to the Arm CMSIS CSolution extentions documentation on how to use the CMSIS View. It offers convenient and pre-configured access to build and debug features.","title":"Build and Run"},{"location":"target_configuration_refapp.html#enable-ethos-u-npu-support-for-your-target","text":"To enable NPU support for custom hardware: Open the *.csolution.yml file and look for the first target-type : target-types: - type: MyCustomBoard device: STM32U585AIIx # Your target MCU variables: ... To the \"type\" entry add a suffix that identifies NPU model and MAC configuration. e.g.: - type: MyCustomBoard-U55-128 # or - type: MyCustomBoard-U85-512 If you do not specify a MAC variant, 256 will be the default. This will configure the project to include the correct drivers and models for the NPU selected.","title":"Enable Ethos-U NPU support for your target"},{"location":"target_configuration_refapp.html#reference-applications","text":"","title":"Reference Applications"},{"location":"target_configuration_refapp.html#audio-template-keyword-spotting","text":"The KWS template demonstrates real-time wake word detection: Key Features: Real-time audio preprocessing (MFCC feature extraction) Optimized neural network inference using CMSIS-NN Configurable wake word models Performance profiling and benchmarking Getting Started: Build and run the template on your target platform Speak the wake word (e.g. \"Yes\" or \"Up\") Observe detection results via UART output or LEDs Replace the model with your custom wake word model Customization Points: kws/src/kws_model.cpp : Replace with your TensorFlow Lite model kws/src/audio_preprocessing.cpp : Modify audio preprocessing pipeline kws/config/kws_config.h : Adjust detection thresholds and parameters","title":"Audio Template: Keyword Spotting"},{"location":"target_configuration_refapp.html#video-template-object-detection","text":"The object detection template provides real-time computer vision: Key Features: Camera input processing and frame buffering Object detection using MobileNet-based models Bounding box visualization Multi-object detection and classification Getting Started: Build and run the template with camera input Point camera at objects for detection View detection results on display or via debug output Integrate your custom object detection model Customization Points: object-detection/src/detection_model.cpp : Replace with your model object-detection/src/image_preprocessing.cpp : Modify image preprocessing object-detection/config/detection_config.h : Adjust detection parameters","title":"Video Template: Object Detection"},{"location":"target_configuration_refapp.html#generic-template-inference-runner","text":"(available in pack version 1.0+) The generic inference template provides maximum flexibility: Key Features: Framework for any TensorFlow Lite model Configurable input/output tensors Performance benchmarking utilities Extensible architecture for custom applications Getting Started: Replace the example model with your TensorFlow Lite model Configure input/output tensor specifications Implement custom preprocessing/postprocessing Build and test your custom ML application Customization Points: inference_runner/Model/ : Replace with your TensorFlow Lite model files inference_runner/src/inference_runner.cpp : Modify inference pipeline inference_runner/config/ : Adjust model and application configuration","title":"Generic Template: Inference Runner"},{"location":"target_configuration_refapp.html#performance-optimization","text":"All MLEK templates include built-in performance optimization features: CMSIS-NN Integration : Optimized neural network kernels for Cortex-M Ethos-U Acceleration : NPU acceleration for supported layers Memory Optimization : Efficient memory management and tensor allocation Profiling Tools : Built-in timing and resource usage measurements","title":"Performance Optimization"},{"location":"target_configuration_refapp.html#next-steps","text":"After successfully running an MLEK template: Model Integration : Replace the example model with your trained TensorFlow Lite model Application Customization : Modify the application logic for your specific use case Performance Tuning : Optimize for your target constraints (memory, power, latency) Hardware Deployment : Test on your target hardware platform Production Deployment : Integrate into your final product design For detailed examples and additional resources, refer to the individual template README files and the MLEK documentation.","title":"Next Steps"},{"location":"templates_audio.html","text":"MLEK - Audio Applications This chapter describes the MLEK reference applications for real-time audio processing: Keyword Spotting (KWS) : Demonstrates wake word detection and voice command recognition Audio User Algorithm Template : Provides a foundation for custom audio ML processing applications Required API Interfaces For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_AUDIO_IN Virtual Audio Input / Audio Interface STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual Keyword Spotting Application Keyword spotting (KWS) reference application detects predefined words or phrases from a continuous audio stream. An embedded device can listen to a list of \"wake words\" and use it to execute commands. This implementation uses TensorFlow Lite Micro and CMSIS-NN for optimized inference on Cortex-M processors or Ethos-U processors. How KWS works: Incoming audio is captured from a microphone or played back from a test sample. The audio stream is preprocessed and converted into Mel-frequency cepstral coefficients (MFCC) features. A MicroNet keyword spotting model classifies the MFCC features to determine which keyword, if any, was spoken. Detection results are reported via printf messages. Audio Stream Preprocessing The MicroNet keyword spotting model expects audio data to be preprocessed before performing an inference. This section is an overview of the feature extraction process used. First, the audio data is normalized to the range (-1, 1). Mel-Frequency Cepstral Coefficients (MFCCs) are a common feature that is extracted from audio data and can be used as input for machine learning tasks such as keyword spotting and speech recognition. For implementation details, please refer Mfcc.cc and Mfcc.hpp . These files are part of the software component ML Eval Kit:Common:API . Next, a window of 640 audio samples (40ms) is taken from the audio input. From these 640 samples, we calculate 10 MFCC features. The whole window is shifted to the right by 320 audio samples (20ms) and 10 new MFCC features are calculated. This process of shifting and calculating is repeated. For 1 second audio input, 49 windows that each have 10 MFCC features are calculated. These extracted features are quantized and an inference is performed. MicroMet ML Model The KWS application uses the MicroNet Medium INT8 model that is trained for twelve keywords (see file src/Labels.cpp ). ToDo: - how to get scripts? - how to convert models? A sample audio file containing the word \"down\" is provided for testing. The picture below shows serial output from a hardware target, while the application detects the keyword \"yes\" on a microphone stream. Build Types The KWS example defines four build types that control debug information and the audio source: Build Type Description Debug-Live_Stream Uses live microphone input with debug information enabled. Release-Live_Stream Live microphone input with optimizations for performance. Debug-Data_Array Processes a built-in audio array for regression testing with debug information. Release-Data_Array Processes the audio array with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time audio or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface Audio User Algorithm Template Todo. Working with MLEK Templates See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"MLEK - Audio Applications"},{"location":"templates_audio.html#mlek-audio-applications","text":"This chapter describes the MLEK reference applications for real-time audio processing: Keyword Spotting (KWS) : Demonstrates wake word detection and voice command recognition Audio User Algorithm Template : Provides a foundation for custom audio ML processing applications","title":"MLEK - Audio Applications"},{"location":"templates_audio.html#required-api-interfaces","text":"For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_AUDIO_IN Virtual Audio Input / Audio Interface STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual","title":"Required API Interfaces"},{"location":"templates_audio.html#keyword-spotting-application","text":"Keyword spotting (KWS) reference application detects predefined words or phrases from a continuous audio stream. An embedded device can listen to a list of \"wake words\" and use it to execute commands. This implementation uses TensorFlow Lite Micro and CMSIS-NN for optimized inference on Cortex-M processors or Ethos-U processors. How KWS works: Incoming audio is captured from a microphone or played back from a test sample. The audio stream is preprocessed and converted into Mel-frequency cepstral coefficients (MFCC) features. A MicroNet keyword spotting model classifies the MFCC features to determine which keyword, if any, was spoken. Detection results are reported via printf messages.","title":"Keyword Spotting Application"},{"location":"templates_audio.html#audio-stream-preprocessing","text":"The MicroNet keyword spotting model expects audio data to be preprocessed before performing an inference. This section is an overview of the feature extraction process used. First, the audio data is normalized to the range (-1, 1). Mel-Frequency Cepstral Coefficients (MFCCs) are a common feature that is extracted from audio data and can be used as input for machine learning tasks such as keyword spotting and speech recognition. For implementation details, please refer Mfcc.cc and Mfcc.hpp . These files are part of the software component ML Eval Kit:Common:API . Next, a window of 640 audio samples (40ms) is taken from the audio input. From these 640 samples, we calculate 10 MFCC features. The whole window is shifted to the right by 320 audio samples (20ms) and 10 new MFCC features are calculated. This process of shifting and calculating is repeated. For 1 second audio input, 49 windows that each have 10 MFCC features are calculated. These extracted features are quantized and an inference is performed.","title":"Audio Stream Preprocessing"},{"location":"templates_audio.html#micromet-ml-model","text":"The KWS application uses the MicroNet Medium INT8 model that is trained for twelve keywords (see file src/Labels.cpp ). ToDo: - how to get scripts? - how to convert models? A sample audio file containing the word \"down\" is provided for testing. The picture below shows serial output from a hardware target, while the application detects the keyword \"yes\" on a microphone stream.","title":"MicroMet ML Model"},{"location":"templates_audio.html#build-types","text":"The KWS example defines four build types that control debug information and the audio source: Build Type Description Debug-Live_Stream Uses live microphone input with debug information enabled. Release-Live_Stream Live microphone input with optimizations for performance. Debug-Data_Array Processes a built-in audio array for regression testing with debug information. Release-Data_Array Processes the audio array with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time audio or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface","title":"Build Types"},{"location":"templates_audio.html#audio-user-algorithm-template","text":"Todo.","title":"Audio User Algorithm Template"},{"location":"templates_audio.html#working-with-mlek-templates","text":"See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"Working with MLEK Templates"},{"location":"templates_video.html","text":"MLEK Video Application This chapter describes the MLEK reference applications for real-time video processing: Video/Object Detection : Object detection using camera input, bounding box data visualization Video/User Algorithm : Provides a foundation for a custom ML algorithms Required API Interfaces For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_VIDEO_IN Virtual Video Input / Camera Interface CMSIS_VSTREAM_VIDEO_OUT Virtual Video Output / Display STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual Object Detection Application This example uses a neural network model that specialises in detecting human faces in images. The input size for these images is 192x192 (monochrome) and the smallest face that can be detected is of size 20x20. The output of the application will be co-ordinates for rectangular bounding boxes for each detection. Build Types The Object Detection example defines four build types that control debug information and the video source: Build Type Description Debug-Live_Stream Capture frames from a camera in real time. Debug information enabled. Release-Live_Stream Capture frames from a camera in real time. With optimizations for performance. Debug-Data_Array Built-in image data for regression testing with debug information. Release-Data_Array Built-in image data with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time video or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface Video User Algorithm Template Todo Working with MLEK Templates See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"MLEK - Video Applications"},{"location":"templates_video.html#mlek-video-application","text":"This chapter describes the MLEK reference applications for real-time video processing: Video/Object Detection : Object detection using camera input, bounding box data visualization Video/User Algorithm : Provides a foundation for a custom ML algorithms","title":"MLEK Video Application"},{"location":"templates_video.html#required-api-interfaces","text":"For hardware deployment, the Board-Layer should provide the following API interfaces: Required API Interface Description CMSIS_VIO Virtual I/O interface for LEDs, buttons, and basic I/O CMSIS_VSTREAM_VIDEO_IN Virtual Video Input / Camera Interface CMSIS_VSTREAM_VIDEO_OUT Virtual Video Output / Display STDOUT, STDERR Standard output for printf debugging and logging These interfaces ideally are supplied by the vendor of your evaluation board. For custom hardware, details on the implementation are found in the CMSIS-Driver Manual","title":"Required API Interfaces"},{"location":"templates_video.html#object-detection-application","text":"This example uses a neural network model that specialises in detecting human faces in images. The input size for these images is 192x192 (monochrome) and the smallest face that can be detected is of size 20x20. The output of the application will be co-ordinates for rectangular bounding boxes for each detection.","title":"Object Detection Application"},{"location":"templates_video.html#build-types","text":"The Object Detection example defines four build types that control debug information and the video source: Build Type Description Debug-Live_Stream Capture frames from a camera in real time. Debug information enabled. Release-Live_Stream Capture frames from a camera in real time. With optimizations for performance. Debug-Data_Array Built-in image data for regression testing with debug information. Release-Data_Array Built-in image data with release optimizations. Use the Debug build types during development and the Release build types for performance measurements. Switch between Live_Stream and Data_Array depending on whether you want real-time video or a fixed sample. On Arm Virtual Hardware Targets, the Live_Stream is utilizing the VSI interface","title":"Build Types"},{"location":"templates_video.html#video-user-algorithm-template","text":"Todo","title":"Video User Algorithm Template"},{"location":"templates_video.html#working-with-mlek-templates","text":"See Target Configuration chapters on how to deploy the reference applications to a specific hardware or simulation target.","title":"Working with MLEK Templates"}]}